{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary packages along the way\n",
    "#loading and cleaning the data\n",
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy as np\n",
    "\n",
    "#NN Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data file\n",
    "def load_file(filename):\n",
    "    #opening the file as read only\n",
    "    file=open(filename, mode='rt', encoding='utf-8')\n",
    "    #reading line by line\n",
    "    text=file.readlines()\n",
    "    #splitting the lines and extract the sentence pairs\n",
    "    pairs=[]\n",
    "    for lines in text:\n",
    "        temp=[]\n",
    "        #line=re.split('[?.!]', lines)\n",
    "        line=lines.split(\"\\t\")\n",
    "        temp.append(line[0])\n",
    "        temp.append(line[1])\n",
    "        pairs.append(temp)\n",
    "    #closing the file\n",
    "    file.close()\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the pairs of sentences from punctuation\n",
    "#normalizing the pairs of sentences to lowercase\n",
    "def clean_pairs(lines):\n",
    "    cleaned=[]\n",
    "    #regex for character filtering\n",
    "    re_print=re.compile('[^%s]' % re.escape(string.printable))\n",
    "    #translation table for removing punctuation\n",
    "    table=str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair=[]\n",
    "        for line in pair:\n",
    "            #tokenize on white space\n",
    "            line=line.split()\n",
    "            #convert to lowercase\n",
    "            line=[word.lower() for word in line]\n",
    "            #remove punctuation from each token\n",
    "            line=[word.translate(table) for word in line]\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return np.array(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the source language is English and the target language is Swedish\n",
    "swedish_english=load_file(\"swe.txt\")\n",
    "#print(swedish)\n",
    "cleaned_swedish=clean_pairs(swedish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the dimension of data\n",
    "#print(cleaned_swedish)\n",
    "#print(len(cleaned_swedish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the cleaned data into train and test\n",
    "data=cleaned_swedish\n",
    "np.random.shuffle(data)\n",
    "#as training a neural network requires an enormous amount of data, we designate 90% of the data to training phase\n",
    "train, test=data[:int(0.9*len(data))], data[int(0.9*len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train)\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting a toknizer\n",
    "def tokenize(lines):\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#maximum sentence length\n",
    "def max_length(lines):\n",
    "    return max(len(line.split()) for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokens=tokenize(data[:, 0])\n",
    "eng_words=len(eng_tokens.word_index)+1\n",
    "eng_length=max_length(data[:, 0])\n",
    "#print(eng_words)\n",
    "#print(eng_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "swe_tokens=tokenize(data[:, 1])\n",
    "swe_words=len(swe_tokens.word_index)+1\n",
    "swe_length=max_length(data[:, 1])\n",
    "#print(swe_words)\n",
    "#print(swe_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode and add padding in source language\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    #integer encode sequences\n",
    "    X=tokenizer.texts_to_sequences(lines)\n",
    "    #pad sequences\n",
    "    X=pad_sequences(X, maxlen=length, padding='post')\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode target sentence\n",
    "def encode_output(sequences, vocab_size):\n",
    "    ylist=[]\n",
    "    for sequence in sequences:\n",
    "        encoded=to_categorical(sequence, num_classes=vocab_size)\n",
    "        ylist.append(encoded)\n",
    "    y=np.array(ylist)\n",
    "    y=y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=encode_sequences(swe_tokens, swe_length, train[:, 1])\n",
    "y_train=encode_sequences(eng_tokens, eng_length, train[:, 0])\n",
    "y_train=encode_output(y_train, eng_words)\n",
    "\n",
    "x_test=encode_sequences(swe_tokens, swe_length, test[:, 1])\n",
    "y_test=encode_sequences(eng_tokens, eng_length, test[:, 0])\n",
    "y_test=encode_output(y_test, eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 29, 256)           1942016   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "repeat_vector_4 (RepeatVecto (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 32, 256)           525312    \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 32, 5273)          1355161   \n",
      "=================================================================\n",
      "Total params: 4,347,801\n",
      "Trainable params: 4,347,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def NN_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
    "    model=Sequential()\n",
    "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(n_units))\n",
    "    model.add(RepeatVector(tar_timesteps))\n",
    "    model.add(LSTM(n_units, return_sequences=True))\n",
    "    model.add(TimeDistributed(Dense(tar_vocab, activation='softmax')))\n",
    "    return model\n",
    " \n",
    "model=NN_model(swe_words, eng_words, swe_length, eng_length, 256)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13624 samples, validate on 3406 samples\n",
      "Epoch 1/30\n",
      " - 486s - loss: 0.8507 - accuracy: 0.8622 - val_loss: 0.9293 - val_accuracy: 0.8577\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.92930, saving model to model1.h5\n",
      "Epoch 2/30\n",
      " - 465s - loss: 0.8223 - accuracy: 0.8653 - val_loss: 0.9112 - val_accuracy: 0.8601\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.92930 to 0.91121, saving model to model1.h5\n",
      "Epoch 3/30\n",
      " - 467s - loss: 0.7903 - accuracy: 0.8695 - val_loss: 0.8909 - val_accuracy: 0.8651\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.91121 to 0.89092, saving model to model1.h5\n",
      "Epoch 4/30\n",
      " - 547s - loss: 0.7584 - accuracy: 0.8737 - val_loss: 0.8647 - val_accuracy: 0.8688\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.89092 to 0.86471, saving model to model1.h5\n",
      "Epoch 5/30\n",
      " - 484s - loss: 0.7251 - accuracy: 0.8777 - val_loss: 0.8471 - val_accuracy: 0.8711\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.86471 to 0.84712, saving model to model1.h5\n",
      "Epoch 6/30\n",
      " - 466s - loss: 0.6931 - accuracy: 0.8812 - val_loss: 0.8333 - val_accuracy: 0.8739\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.84712 to 0.83332, saving model to model1.h5\n",
      "Epoch 7/30\n",
      " - 484s - loss: 0.6607 - accuracy: 0.8842 - val_loss: 0.8094 - val_accuracy: 0.8756\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.83332 to 0.80940, saving model to model1.h5\n",
      "Epoch 8/30\n",
      " - 484s - loss: 0.6286 - accuracy: 0.8876 - val_loss: 0.7938 - val_accuracy: 0.8782\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.80940 to 0.79382, saving model to model1.h5\n",
      "Epoch 9/30\n",
      " - 482s - loss: 0.5997 - accuracy: 0.8902 - val_loss: 0.7848 - val_accuracy: 0.8790\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.79382 to 0.78476, saving model to model1.h5\n",
      "Epoch 10/30\n",
      " - 477s - loss: 0.5695 - accuracy: 0.8934 - val_loss: 0.7718 - val_accuracy: 0.8823\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.78476 to 0.77177, saving model to model1.h5\n",
      "Epoch 11/30\n",
      " - 476s - loss: 0.5419 - accuracy: 0.8963 - val_loss: 0.7573 - val_accuracy: 0.8833\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.77177 to 0.75730, saving model to model1.h5\n",
      "Epoch 12/30\n",
      " - 475s - loss: 0.5111 - accuracy: 0.9001 - val_loss: 0.7519 - val_accuracy: 0.8843\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.75730 to 0.75189, saving model to model1.h5\n",
      "Epoch 13/30\n",
      " - 461s - loss: 0.4845 - accuracy: 0.9034 - val_loss: 0.7415 - val_accuracy: 0.8867\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.75189 to 0.74151, saving model to model1.h5\n",
      "Epoch 14/30\n",
      " - 460s - loss: 0.4568 - accuracy: 0.9073 - val_loss: 0.7322 - val_accuracy: 0.8875\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.74151 to 0.73221, saving model to model1.h5\n",
      "Epoch 15/30\n",
      " - 461s - loss: 0.4289 - accuracy: 0.9110 - val_loss: 0.7220 - val_accuracy: 0.8882\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.73221 to 0.72202, saving model to model1.h5\n",
      "Epoch 16/30\n",
      " - 465s - loss: 0.4042 - accuracy: 0.9149 - val_loss: 0.7164 - val_accuracy: 0.8901\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.72202 to 0.71645, saving model to model1.h5\n",
      "Epoch 17/30\n",
      " - 459s - loss: 0.3808 - accuracy: 0.9186 - val_loss: 0.7148 - val_accuracy: 0.8910\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.71645 to 0.71477, saving model to model1.h5\n",
      "Epoch 18/30\n",
      " - 461s - loss: 0.3559 - accuracy: 0.9227 - val_loss: 0.7110 - val_accuracy: 0.8916\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.71477 to 0.71097, saving model to model1.h5\n",
      "Epoch 19/30\n",
      " - 463s - loss: 0.3335 - accuracy: 0.9273 - val_loss: 0.7050 - val_accuracy: 0.8907\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.71097 to 0.70503, saving model to model1.h5\n",
      "Epoch 20/30\n",
      " - 466s - loss: 0.3130 - accuracy: 0.9310 - val_loss: 0.7045 - val_accuracy: 0.8937\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.70503 to 0.70454, saving model to model1.h5\n",
      "Epoch 21/30\n",
      " - 469s - loss: 0.2938 - accuracy: 0.9350 - val_loss: 0.7062 - val_accuracy: 0.8942\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.70454\n",
      "Epoch 22/30\n",
      " - 460s - loss: 0.2770 - accuracy: 0.9383 - val_loss: 0.7011 - val_accuracy: 0.8948\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.70454 to 0.70114, saving model to model1.h5\n",
      "Epoch 23/30\n",
      " - 455s - loss: 0.2600 - accuracy: 0.9416 - val_loss: 0.7015 - val_accuracy: 0.8952\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.70114\n",
      "Epoch 24/30\n",
      " - 456s - loss: 0.2438 - accuracy: 0.9448 - val_loss: 0.7023 - val_accuracy: 0.8960\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.70114\n",
      "Epoch 25/30\n",
      " - 461s - loss: 0.2283 - accuracy: 0.9481 - val_loss: 0.7005 - val_accuracy: 0.8948\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.70114 to 0.70051, saving model to model1.h5\n",
      "Epoch 26/30\n",
      " - 456s - loss: 0.2135 - accuracy: 0.9514 - val_loss: 0.7037 - val_accuracy: 0.8973\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.70051\n",
      "Epoch 27/30\n",
      " - 462s - loss: 0.2027 - accuracy: 0.9534 - val_loss: 0.7065 - val_accuracy: 0.8976\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.70051\n",
      "Epoch 28/30\n",
      " - 463s - loss: 0.1906 - accuracy: 0.9560 - val_loss: 0.7039 - val_accuracy: 0.8976\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.70051\n",
      "Epoch 29/30\n",
      " - 458s - loss: 0.1780 - accuracy: 0.9588 - val_loss: 0.7056 - val_accuracy: 0.8977\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.70051\n",
      "Epoch 30/30\n",
      " - 460s - loss: 0.1684 - accuracy: 0.9607 - val_loss: 0.7065 - val_accuracy: 0.8976\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.70051\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ff07de48128>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename='model1.h5'\n",
    "checkpoint=ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "model.fit(x_train, y_train, epochs=30, batch_size=64, validation_split=0.2, callbacks=[checkpoint], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensorflow_env/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swedish: jag har ett barn, English: i have a kid, NN: i have a book\n",
      "\n",
      "Swedish: mary satte några blommor i vasen och ställde sedan vasen på bordet, English: mary put some flowers in the vase and then put the vase on the table, NN: he put some some the to to to the the\n",
      "\n",
      "Swedish: är det blått, English: is it blue, NN: is it\n",
      "\n",
      "Swedish: hade tom sett något skulle han ha sagt det till oss, English: if tom had seen anything he wouldve told us, NN: has tom tom tom seen tom tom tom to\n",
      "\n",
      "Swedish: är ni två fulla, English: are you both drunk, NN: are you are tonight\n",
      "\n",
      "Swedish: tom köpte en lädersoffa, English: tom bought a leather couch, NN: tom bought a speeding\n",
      "\n",
      "Swedish: vill du ha fisk, English: do you want fish, NN: do you want food\n",
      "\n",
      "Swedish: som jag sa jag var upptagen, English: like i said i was busy, NN: i i think i was was\n",
      "\n",
      "Swedish: köp lite godis åt tom bara, English: just buy tom some candy, NN: dont give tom some dog\n",
      "\n",
      "Swedish: har tom kabeltv, English: does tom have cable, NN: has tom been sons\n",
      "\n",
      "BLEU-1: 0.476362\n",
      "BLEU-2: 0.332096\n",
      "BLEU-3: 0.271129\n",
      "BLEU-4: 0.164066\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "#map encodings to words\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index==integer:\n",
    "            return word\n",
    "    return None\n",
    " \n",
    "#generate target sequence given source sequence\n",
    "def predict_sequence(model, tokenizer, source):\n",
    "    prediction=model.predict(source, verbose=0)[0]\n",
    "    integers=[np.argmax(vector) for vector in prediction]\n",
    "    target=list()\n",
    "    for i in integers:\n",
    "        word=word_for_id(i, tokenizer)\n",
    "        if word is None:\n",
    "            break\n",
    "        target.append(word)\n",
    "    return ' '.join(target)\n",
    " \n",
    "#evaluate the model\n",
    "def evaluate_model(model, tokenizer, sources, raw_dataset):\n",
    "    actual, predicted=[], []\n",
    "    for i, source in enumerate(sources):\n",
    "        #translate encoded source text\n",
    "        source=source.reshape((1, source.shape[0]))\n",
    "        translation=predict_sequence(model, eng_tokens, source)\n",
    "        raw_target, raw_src=raw_dataset[i]\n",
    "        if i<10:\n",
    "            print('Swedish: %s, English: %s, NN: %s' % (raw_src, raw_target, translation))\n",
    "            print()\n",
    "        actual.append([raw_target.split()])\n",
    "        predicted.append(translation.split())\n",
    "    #calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    " \n",
    "\n",
    "#load model\n",
    "model=load_model('model1.h5')\n",
    "#test on some test sequences\n",
    "evaluate_model(model, eng_tokens, x_test, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
