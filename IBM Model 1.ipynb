{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "n2zRanbfnEkU"
   },
   "outputs": [],
   "source": [
    "#importing the necessary libraries along the way\n",
    "from collections import Counter,defaultdict\n",
    "#we need this to clean the data from punctuation and unwanted stuff\n",
    "import re\n",
    "from math import log as l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data files\n",
    "#English\n",
    "import urllib.request\n",
    "en_url=\"https://drive.google.com/uc?export=download&id=1hOw7FCT5cmMjlWdrA88Dg6whe7_Blx7B\"\n",
    "en_file=urllib.request.urlopen(en_url)\n",
    "#more and more cleaning\n",
    "en=re.findall(r'\\w+', en_file.read().decode(\"utf-8\"))\n",
    "count_en=Counter(en)\n",
    "\n",
    "#German\n",
    "de_file=open(\"europarl-v7.de-en.lc.de\", \"r\", encoding=\"utf-8\").read()\n",
    "#more and more cleaning\n",
    "de=re.findall(r'\\w+', de_file)\n",
    "\n",
    "#French\n",
    "fr_file=open(\"europarl-v7.fr-en.lc.fr\", \"r\", encoding=\"utf-8\").read()\n",
    "#more and more cleaning\n",
    "fr=re.findall(r'\\w+', fr_file)\n",
    "\n",
    "#Swedish\n",
    "sv_url=\"https://drive.google.com/uc?export=download&id=1b0CvmX80SLYcBkbKBz3DU0o2dBwfqFoI\"\n",
    "sv_file=urllib.request.urlopen(sv_url)\n",
    "#more and more cleaning\n",
    "sv=re.findall(r'\\w+', sv_file.read().decode(\"utf-8\"))\n",
    "count_sv=Counter(sv)\n",
    "\n",
    "#European Parliment Proceedings aka europe\n",
    "europe=en+de+fr+sv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BcKfknEh5SaO"
   },
   "source": [
    "(a) Warmup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ouA4eqawki7p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most frequent words in English are:\n",
      "[('the', 19327), ('of', 9344), ('to', 8814), ('and', 6949), ('in', 6124), ('is', 4400), ('that', 4357), ('a', 4271), ('we', 3223), ('this', 3222)]\n",
      "\n",
      "The 10 most frequent words in German are:\n",
      "[('die', 10521), ('der', 9374), ('und', 7028), ('in', 4175), ('zu', 3169), ('den', 2976), ('wir', 2863), ('daß', 2738), ('ich', 2670), ('das', 2669)]\n",
      "\n",
      "The 10 most frequent words in French are:\n",
      "[('apos', 16729), ('de', 14528), ('la', 9746), ('et', 6620), ('l', 6536), ('le', 6177), ('à', 5588), ('les', 5587), ('des', 5232), ('que', 4797)]\n",
      "\n",
      "The 10 most frequent words in Swedish are:\n",
      "[('att', 9181), ('och', 7038), ('i', 5954), ('det', 5687), ('som', 5028), ('för', 4959), ('av', 4013), ('är', 3840), ('en', 3724), ('vi', 3211)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#importing the library to count the common words\n",
    "from collections import Counter\n",
    "\n",
    "#the 10 most frequent words in each language\n",
    "#English\n",
    "print(\"The 10 most frequent words in English are:\")\n",
    "print(Counter(en).most_common(10))\n",
    "print()\n",
    "\n",
    "#German\n",
    "print(\"The 10 most frequent words in German are:\")\n",
    "print(Counter(de).most_common(10))\n",
    "print()\n",
    "\n",
    "#French\n",
    "print(\"The 10 most frequent words in French are:\")\n",
    "print(Counter(fr).most_common(10))\n",
    "print()\n",
    "\n",
    "#Swedish\n",
    "print(\"The 10 most frequent words in Swedish are:\")\n",
    "print(Counter(sv).most_common(10))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Rddd6b2KnCSm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the word speaker is 9.740636083017494e-06\n",
      "\n",
      "The probability of the word zebra is 0.0\n"
     ]
    }
   ],
   "source": [
    "#The probabilities are calculated as the frequency of a given word divide by the total number of words\n",
    "count_europe=Counter(europe)\n",
    "\n",
    "#speaker\n",
    "print(\"The probability of the word speaker is\", count_europe[\"speaker\"]/sum(count_europe.values()))\n",
    "\n",
    "#for a nicer output\n",
    "print()\n",
    "\n",
    "#Zebra\n",
    "print(\"The probability of the word zebra is\", count_europe[\"zebra\"]/sum(count_europe.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUl5XUaX5KKM"
   },
   "source": [
    "(b) Language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "em0nu6K85nhe"
   },
   "outputs": [],
   "source": [
    "def frequency(url):\n",
    "  frequencies={}\n",
    "  file=urllib.request.urlopen(url)\n",
    "  for line in file:\n",
    "    words=re.findall(r'\\w+', line.decode(\"utf-8\"))\n",
    "    frequencies=finding_bigrams(words, frequencies)\n",
    "  return frequencies\n",
    "\n",
    "def finding_bigrams(words, bigrams={}):\n",
    "  for i in range(len(words)-1):\n",
    "    bigram=(words[i], words[i+1])\n",
    "    if not bigram in bigrams:\n",
    "      bigrams[bigram]=1\n",
    "    else:\n",
    "      bigrams[bigram]+=1\n",
    "  return bigrams\n",
    "\n",
    "def probabilities(count_bigrams, count_words):\n",
    "  probabilities={}\n",
    "  for bigram, count in count_bigrams.items(): \n",
    "    word=bigram[0]\n",
    "    probabilities[bigram]=count/count_words[word]\n",
    "  return probabilities\n",
    "\n",
    "def bigram_language_model(bigrams_probabilities, count_bigrams, laplace_smoothing):\n",
    "  prob=1\n",
    "  #prob=l(1)\n",
    "  for bigram, count in count_bigrams.items():\n",
    "    if bigram in bigrams_probabilities:\n",
    "      prob=prob*bigrams_probabilities[bigram]\n",
    "      #prob=prob+l(bigrams_probabilities[bigram])\n",
    "    else:\n",
    "      prob=prob*laplace_smoothing\n",
    "      #prob=prob+l(laplace_smoothing)\n",
    "  return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "1w43kYXCYvGb"
   },
   "outputs": [],
   "source": [
    "en_bigrams=frequency(en_url)\n",
    "probs=probabilities(en_bigrams, count_en)\n",
    "pseudocount=1/(sum(en_bigrams.values())+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous part (a), we observed that the probability of a word which did not appear in the text was zero. To prevent any sequential troubles we used Adaptive Smoothing also known as Laplace Smoothing and added a pseudocount to the probabilities so it won't be zero! Furthermore, to compute the probabilities for individual long English sentences, we can use logarithmic probabilities instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nh5CxU2jdwQe",
    "outputId": "2aa01e1c-9d32-4340-aca6-ea7f37b39c28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.736539063879752e-37\n"
     ]
    }
   ],
   "source": [
    "#computing the probability of a short random sentence irrelevant to the training data\n",
    "sentence=\"she was gone and yet she was more present than anyone else\"\n",
    "words=re.findall(r'\\w+', sentence)\n",
    "sentence_bigrams=finding_bigrams(words)\n",
    "bigrams=finding_bigrams(words)\n",
    "print(bigram_language_model(probs, bigrams, pseudocount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0D3RdxxjpcM",
    "outputId": "747738f0-5417-4a68-8df7-85eeb2e04933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.881798402577779e-46\n"
     ]
    }
   ],
   "source": [
    "#this one is obtained from the training text\n",
    "sentence=\"it is the case of alexander nikitin\"\n",
    "words=re.findall(r'\\w+', sentence)\n",
    "sentence_bigrams=finding_bigrams(words)\n",
    "bigrams=finding_bigrams(words)\n",
    "print(bigram_language_model(probs, bigrams, pseudocount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MiLw5VPwihj9"
   },
   "source": [
    "(c) Translation modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "tE8LXK--1vbL"
   },
   "outputs": [],
   "source": [
    "def split_sentences(url):\n",
    "  file=urllib.request.urlopen(url)\n",
    "  sentences=list()\n",
    "  for line in file:\n",
    "    sentences.append(line.decode(\"utf-8\"))\n",
    "  return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "oIdGYOovSXfB"
   },
   "outputs": [],
   "source": [
    "#we need a flag or etc to point the beginning/ending of a sentence\n",
    "flag=\"null\"\n",
    "en_sentences=split_sentences(en_url)\n",
    "sv_sentences=split_sentences(sv_url)\n",
    "en.append(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "uEaS53RsSld-"
   },
   "outputs": [],
   "source": [
    "transition_probabilities=dict()\n",
    "#default value provided as uniform probability\n",
    "for i in range(0, len(en_sentences)):\n",
    "  en_words=re.findall(r'\\w+', en_sentences[i])\n",
    "  en_words.append(flag)\n",
    "  sv_words=re.findall(r'\\w+', sv_sentences[i])\n",
    "  for j in sv_words:\n",
    "    for k in en_words:\n",
    "      transition_probabilities[(j,k)]=1.0/len(en_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7sqxbKbw1vbP",
    "outputId": "de6ee4ae-039b-4020-e29b-00afa1a90ef6"
   },
   "outputs": [],
   "source": [
    "number_of_iterations=10\n",
    "\n",
    "for i in range(0, number_of_iterations):\n",
    "  count=defaultdict(float)\n",
    "  es_total=defaultdict(float)\n",
    "  total=defaultdict(float)\n",
    "  for j in range(len(sv_sentences)):\n",
    "    es=re.findall(r'\\w+', en_sentences[j])\n",
    "    es.append(flag)\n",
    "    fs=re.findall(r'\\w+', sv_sentences[j])\n",
    "    #compute normalization\n",
    "    for f in fs:\n",
    "      total[f]=0.0\n",
    "      for e in es:\n",
    "        total[f]+=transition_probabilities[(f, e)]\n",
    "    for f in fs:\n",
    "      for e in es:\n",
    "        delta=transition_probabilities[(f, e)]/total[f]\n",
    "        count[(f, e)]+=delta\n",
    "        es_total[e]+=delta\n",
    "  for j in range(len(sv_sentences)):\n",
    "    es=re.findall(r'\\w+', en_sentences[j])\n",
    "    es.append(flag)\n",
    "    fs=re.findall(r'\\w+', sv_sentences[j])\n",
    "    #estimate probability\n",
    "    for f in fs:\n",
    "      for e in es:\n",
    "        transition_probabilities[(f, e)]=count[(f, e)]/es_total[e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('europeiska', 'european'): 0.8450574393627105, ('europeisk', 'european'): 0.08375866478837797, ('den', 'european'): 0.01152108595074964, ('i', 'european'): 0.010893956404072248, ('att', 'european'): 0.00610113029186832, ('en', 'european'): 0.0055281168349736575, ('till', 'european'): 0.005247648979045568, ('och', 'european'): 0.004863528509948655, ('det', 'european'): 0.004710207166560048, ('för', 'european'): 0.00398211502502309}\n"
     ]
    }
   ],
   "source": [
    "#we need this for the process of sorting the probabilities as we did it wrong previously\n",
    "import heapq\n",
    "from operator import itemgetter\n",
    "\n",
    "testing={k:v for (k,v) in transition_probabilities.items() if k[1]==\"european\"}\n",
    "print(dict(heapq.nlargest(10, testing.items(), key=itemgetter(1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoLiIBUFJ16X"
   },
   "source": [
    "(d) Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to decode the sentence from Swedish to English (or any other source and target language, though we only chose Swedish as we knew it) we first find the highest transition probability for each word in the sentence. Second, using the library itertools, take all the possible permutations into account. And at last, find the sentence with the highest probability among all.\n",
    "The problem is for short sentences with almost 3 sentences it works quite well, but as the number of words increase and the sentence becomes longer the number of permutations also increase. This makes the decoding process to reqire more computational power. Therefore, one mighty solution is to consider n-most common translations to each word!\n",
    "After all, decoding three sentences from the Swedish training data, the model translate those to English with mean accuracy around 85% which could be a good baseline for other advanced available models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "wV1whXGRFAZH"
   },
   "outputs": [],
   "source": [
    "def translation(sv):\n",
    "  t={k:v for (k,v) in transition_probabilities.items() if k[0]==sv}\n",
    "  sorting=heapq.nlargest(1, t.items(), key=itemgetter(1))\n",
    "  #return sorting[0][0][1:n]\n",
    "  return sorting[0][0][1]\n",
    "\n",
    "def permute(words):\n",
    "  #we need this to check the possible permutations for translation\n",
    "  from itertools import permutations\n",
    "  return [list(p) for p in permutations(words)]\n",
    "\n",
    "def max_prob(possible_orders):\n",
    "  prob=0.0\n",
    "  sentence={}\n",
    "  for order in possible_orders:\n",
    "    cb=finding_bigrams(order)\n",
    "    cbp=bigram_language_model(probs, cb, pseudocount)\n",
    "    if cbp>prob:\n",
    "      prob=cbp\n",
    "      sentence=order\n",
    "  return sentence\n",
    "\n",
    "def decode(sv_sentence):\n",
    "  fs=re.findall(r'\\w+', sv_sentence)\n",
    "  translated=list()\n",
    "  for f in fs:\n",
    "    translated.append(translation(f))\n",
    "  possible_orders=permute(translated)\n",
    "  final_translation=max_prob(possible_orders)\n",
    "  return ' '.join(final_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-aekbkd2Qt_n",
    "outputId": "3ade2478-5225-4fdb-a14c-25c7b187a51b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'but i is good'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It is like a hello world! but in a different way :D\n",
    "decode(\"men jag är bra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you familiar returned from media'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(\"ni känner till från media\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'based oz discredited politics'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(\"på den skamliga politik\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
